{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras import optimizers\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import backend as K\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "width = 50\n",
    "height = 10\n",
    "channels = 3\n",
    "classes = 4\n",
    "epochs = 100\n",
    "dataset_dir = '/data/user/adipilat/ParticleID/genEvts/'\n",
    "save_dir = '/data/user/adipilat/ParticleID/models/'\n",
    "padding = 'padding' + str(height)\n",
    "model_name= padding +'_model'\n",
    "history_name = padding + '_history'\n",
    "\n",
    "# This dictionary should be extended to new classes and antiparticles\n",
    "class_labels = {22:0, 11:1, 13:2, 211:3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file gamma.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|██▌       | 1/4 [11:01<33:04, 661.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File gamma.h5  processed\n",
      "Reading file electron.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 2/4 [21:59<22:00, 660.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File electron.h5  processed\n",
      "Reading file muon.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 75%|███████▌  | 3/4 [25:42<08:49, 529.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File muon.h5  processed\n",
      "Reading file pion_c.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 4/4 [36:57<00:00, 573.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File pion_c.h5  processed\n"
     ]
    }
   ],
   "source": [
    "# arrays of data needed for training\n",
    "\n",
    "data_array = []\n",
    "pid_array = []\n",
    "en_array = []\n",
    "\n",
    "# read dataset\n",
    "files = [f for f in os.listdir(dataset_dir) if f.endswith(\"h5\")]\n",
    "\n",
    "for name in tqdm(files):\n",
    "    print(\"Reading file\", name)\n",
    "    data = pd.read_hdf(dataset_dir + name)\n",
    "    n_events = int(0.9 * data.event.max()) # using 90% of events for training\n",
    "\n",
    "    for i in range(1, int(n_events+1)):\n",
    "        tracksters = data.loc[(data['event'] == float(i)) & (data['trackster'] != float(0))]\n",
    "        n_tracksters = tracksters.trackster.max()\n",
    "        lead_en = 0\n",
    "        for j in range(1, int(n_tracksters)+1):\n",
    "            layerclusters = tracksters.loc[tracksters['trackster'] == float(j)]\n",
    "            en = np.sum(layerclusters[\"E\"].values)\n",
    "            if(en>lead_en): #### since I shooted a single particle, only the leading trackster is considered\n",
    "                lead_en = en\n",
    "                image = np.zeros(width*height*channels).reshape(width,height,channels)\n",
    "                pid = int(layerclusters[\"pid\"].iloc[0])\n",
    "                pid = class_labels[pid]\n",
    "                en_value = layerclusters[\"genE\"].iloc[0]\n",
    "                for k in range(1, width+1):\n",
    "                    layer = layerclusters[layerclusters['layer'] == float(k)]\n",
    "                    if(len(layer) != 0):\n",
    "                        temp = layer.E.values, layer.eta.values, layer.phi.values\n",
    "                        temp = np.array(temp).T\n",
    "                        dim = min(temp.shape[0],height)\n",
    "                        image[k-1][:dim] = temp[:dim]\n",
    "        data_array.append(image)\n",
    "        pid_array.append(pid)\n",
    "        en_array.append(en_value)    \n",
    "\n",
    "    print(\"File\", name, \" processed\")\n",
    "\n",
    "data_array = np.array(data_array)\n",
    "pid_array = np.array(pid_array)\n",
    "pid_array = keras.utils.to_categorical(pid_array, num_classes=classes, dtype='float32')\n",
    "en_array = np.array(en_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36000, 50, 10, 3)\n",
      "(36000,)\n",
      "(36000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(data_array.shape)\n",
    "print(en_array.shape)\n",
    "print(pid_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Energy Value: 213.90352475881576\n",
      "Std Energy Value: 108.05413626100672\n"
     ]
    }
   ],
   "source": [
    "####### NORMALIZE THE ENERGY ########\n",
    "\n",
    "mean_en = np.mean(en_array)\n",
    "std_en = np.std(en_array)\n",
    "print('Mean Energy Value: {}'.format(mean_en))\n",
    "print('Std Energy Value: {}'.format(std_en))\n",
    "\n",
    "en_array_norm = (en_array - mean_en)/std_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 50, 10, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 50, 10, 3)    48          input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 50, 10, 3)    84          conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 50, 10, 3)    84          conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 1500)         0           conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 1024)         1537024     flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 256)          262400      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_id1 (Dense)               (None, 64)           16448       dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_er1 (Dense)               (None, 64)           16448       dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_id2 (Dense)               (None, 16)           1040        dense_id1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_er2 (Dense)               (None, 8)            520         dense_er1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pid_output (Dense)              (None, 4)            68          dense_id2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "enreg_output (Dense)            (None, 1)            9           dense_er2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,834,173\n",
      "Trainable params: 1,834,173\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model created!\n"
     ]
    }
   ],
   "source": [
    "print('Creating model...')\n",
    "\n",
    "def tree_model():\n",
    "\n",
    "    input_img = Input(shape=(width, height, channels), name='input')\n",
    "    \n",
    "    conv = Conv2D(3, (5,1), activation='relu', padding='same', kernel_initializer='random_uniform', data_format='channels_last', name='conv1')(input_img)\n",
    "    conv = Conv2D(3, (3,3), activation='relu', padding='same', kernel_initializer='random_uniform', data_format='channels_last', name='conv2')(conv)\n",
    "    conv = Conv2D(3, (3,3), activation='relu', padding='same', kernel_initializer='random_uniform', data_format='channels_last', name='conv3')(conv)\n",
    "\n",
    "    flat = Flatten()(conv)\n",
    "\n",
    "    dense = Dense(1024, activation='relu', kernel_initializer='random_uniform', name='dense1')(flat)\n",
    "    dense = Dense(256, activation='relu', kernel_initializer='random_uniform', name='dense2')(dense)\n",
    "\n",
    "    dense_id = Dense(64, activation='relu', kernel_initializer='random_uniform', name='dense_id1')(dense)\n",
    "    dense_id = Dense(16, activation='relu', kernel_initializer='random_uniform', name='dense_id2')(dense_id)\n",
    "    pid = Dense(classes, activation='softmax', kernel_initializer='random_uniform', name='pid_output')(dense_id)\n",
    "\n",
    "    dense_er = Dense(64, activation='relu', kernel_initializer='random_uniform', name='dense_er1')(dense)\n",
    "    dense_er = Dense(8, activation='relu', kernel_initializer='random_uniform', name='dense_er2')(dense_er)\n",
    "    enreg = Dense(1, name='enreg_output')(dense_er)\n",
    "\n",
    "    model = Model(inputs=input_img, outputs=[pid, enreg])\n",
    "\n",
    "    model.compile(loss={'pid_output': 'categorical_crossentropy', 'enreg_output': 'mse'}, loss_weights={'pid_output': 1, 'enreg_output': 2}, optimizer='adam', metrics={'pid_output': 'accuracy', 'enreg_output': 'mse'})\n",
    "    return model\n",
    "\n",
    "model = tree_model()\n",
    "model.summary()\n",
    "\n",
    "print(\"Model created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32400 samples, validate on 3600 samples\n",
      "Epoch 1/100\n",
      "32400/32400 [==============================] - 2s 53us/step - loss: 2.4003 - pid_output_loss: 1.0522 - enreg_output_loss: 0.6741 - pid_output_acc: 0.4672 - enreg_output_mean_squared_error: 0.6741 - val_loss: 0.8866 - val_pid_output_loss: 0.2756 - val_enreg_output_loss: 0.3055 - val_pid_output_acc: 0.9186 - val_enreg_output_mean_squared_error: 0.3055\n",
      "Epoch 2/100\n",
      "32400/32400 [==============================] - 1s 30us/step - loss: 1.2497 - pid_output_loss: 0.4436 - enreg_output_loss: 0.4030 - pid_output_acc: 0.7250 - enreg_output_mean_squared_error: 0.4030 - val_loss: 0.7152 - val_pid_output_loss: 0.2055 - val_enreg_output_loss: 0.2549 - val_pid_output_acc: 0.9400 - val_enreg_output_mean_squared_error: 0.2549\n",
      "Epoch 3/100\n",
      "32400/32400 [==============================] - 1s 29us/step - loss: 1.1581 - pid_output_loss: 0.4240 - enreg_output_loss: 0.3670 - pid_output_acc: 0.7481 - enreg_output_mean_squared_error: 0.3670 - val_loss: 0.7247 - val_pid_output_loss: 0.1277 - val_enreg_output_loss: 0.2985 - val_pid_output_acc: 0.9622 - val_enreg_output_mean_squared_error: 0.2985\n",
      "Epoch 4/100\n",
      "32400/32400 [==============================] - 1s 30us/step - loss: 1.1262 - pid_output_loss: 0.4151 - enreg_output_loss: 0.3555 - pid_output_acc: 0.7553 - enreg_output_mean_squared_error: 0.3555 - val_loss: 0.5517 - val_pid_output_loss: 0.1082 - val_enreg_output_loss: 0.2218 - val_pid_output_acc: 0.9686 - val_enreg_output_mean_squared_error: 0.2218\n",
      "Epoch 5/100\n",
      "32400/32400 [==============================] - 1s 30us/step - loss: 1.0987 - pid_output_loss: 0.4093 - enreg_output_loss: 0.3447 - pid_output_acc: 0.7624 - enreg_output_mean_squared_error: 0.3447 - val_loss: 0.5641 - val_pid_output_loss: 0.0936 - val_enreg_output_loss: 0.2353 - val_pid_output_acc: 0.9736 - val_enreg_output_mean_squared_error: 0.2353\n",
      "Epoch 6/100\n",
      "32400/32400 [==============================] - 1s 30us/step - loss: 1.0805 - pid_output_loss: 0.4080 - enreg_output_loss: 0.3363 - pid_output_acc: 0.7623 - enreg_output_mean_squared_error: 0.3363 - val_loss: 0.5128 - val_pid_output_loss: 0.0808 - val_enreg_output_loss: 0.2160 - val_pid_output_acc: 0.9758 - val_enreg_output_mean_squared_error: 0.2160\n",
      "Epoch 7/100\n",
      "32400/32400 [==============================] - 1s 30us/step - loss: 1.0705 - pid_output_loss: 0.4035 - enreg_output_loss: 0.3335 - pid_output_acc: 0.7656 - enreg_output_mean_squared_error: 0.3335 - val_loss: 0.5572 - val_pid_output_loss: 0.1360 - val_enreg_output_loss: 0.2106 - val_pid_output_acc: 0.9658 - val_enreg_output_mean_squared_error: 0.2106\n",
      "Epoch 8/100\n",
      "32400/32400 [==============================] - 1s 30us/step - loss: 1.0592 - pid_output_loss: 0.4034 - enreg_output_loss: 0.3279 - pid_output_acc: 0.7669 - enreg_output_mean_squared_error: 0.3279 - val_loss: 0.5523 - val_pid_output_loss: 0.1114 - val_enreg_output_loss: 0.2204 - val_pid_output_acc: 0.9717 - val_enreg_output_mean_squared_error: 0.2204\n",
      "Epoch 9/100\n",
      "32400/32400 [==============================] - 1s 30us/step - loss: 1.0533 - pid_output_loss: 0.4005 - enreg_output_loss: 0.3264 - pid_output_acc: 0.7690 - enreg_output_mean_squared_error: 0.3264 - val_loss: 0.5683 - val_pid_output_loss: 0.0876 - val_enreg_output_loss: 0.2403 - val_pid_output_acc: 0.9758 - val_enreg_output_mean_squared_error: 0.2403\n",
      "Epoch 10/100\n",
      "32400/32400 [==============================] - 1s 30us/step - loss: 1.0445 - pid_output_loss: 0.4002 - enreg_output_loss: 0.3222 - pid_output_acc: 0.7706 - enreg_output_mean_squared_error: 0.3222 - val_loss: 0.5534 - val_pid_output_loss: 0.1328 - val_enreg_output_loss: 0.2103 - val_pid_output_acc: 0.9669 - val_enreg_output_mean_squared_error: 0.2103\n",
      "Epoch 11/100\n",
      "32400/32400 [==============================] - 1s 30us/step - loss: 1.0413 - pid_output_loss: 0.3982 - enreg_output_loss: 0.3216 - pid_output_acc: 0.7727 - enreg_output_mean_squared_error: 0.3216 - val_loss: 0.5493 - val_pid_output_loss: 0.0920 - val_enreg_output_loss: 0.2286 - val_pid_output_acc: 0.9753 - val_enreg_output_mean_squared_error: 0.2286\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data_array, {'pid_output': pid_array, 'enreg_output': en_array_norm}, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)], shuffle=True, verbose=1)\n",
    "history_save = pd.DataFrame(history.history).to_hdf(save_dir + history_name + \".h5\", \"history\", append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /data/user/adipilat/ParticleID/models/ \n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "model.save(save_dir + model_name + \".h5\")\n",
    "print('Saved trained model at %s ' % save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 508 variables.\n",
      "INFO:tensorflow:Converted 508 variables to const ops.\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# save the frozen model\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = convert_variables_to_constants(session, input_graph_def,\n",
    "                                                      output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "\n",
    "frozen_graph = freeze_session(K.get_session(),\n",
    "                              output_names=[out.op.name for out in model.outputs])\n",
    "tf.train.write_graph(frozen_graph, save_dir, model_name + \".pbtxt\", as_text=True)\n",
    "tf.train.write_graph(frozen_graph, save_dir, model_name + \".pb\", as_text=False)\n",
    "print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
